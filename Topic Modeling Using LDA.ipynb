{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efficient-cycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "## To work with the data\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "plastic-hearts",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's read our data\n",
    "df = pd.read_csv(\"consumer_compliants.zip\", compression = \"zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "uniform-decrease",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date received</th>\n",
       "      <th>Product</th>\n",
       "      <th>Sub-product</th>\n",
       "      <th>Issue</th>\n",
       "      <th>Sub-issue</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "      <th>Company public response</th>\n",
       "      <th>Company</th>\n",
       "      <th>State</th>\n",
       "      <th>ZIP code</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Consumer consent provided?</th>\n",
       "      <th>Submitted via</th>\n",
       "      <th>Date sent to company</th>\n",
       "      <th>Company response to consumer</th>\n",
       "      <th>Timely response?</th>\n",
       "      <th>Consumer disputed?</th>\n",
       "      <th>Complaint ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4/3/2020</td>\n",
       "      <td>Vehicle loan or lease</td>\n",
       "      <td>Loan</td>\n",
       "      <td>Getting a loan or lease</td>\n",
       "      <td>Fraudulent loan</td>\n",
       "      <td>This auto loan was opened on XX/XX/2020 in XXX...</td>\n",
       "      <td>Company has responded to the consumer and the ...</td>\n",
       "      <td>TRUIST FINANCIAL CORPORATION</td>\n",
       "      <td>PA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Consent provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>4/3/2020</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3591341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3/12/2020</td>\n",
       "      <td>Debt collection</td>\n",
       "      <td>Payday loan debt</td>\n",
       "      <td>Attempts to collect debt not owed</td>\n",
       "      <td>Debt is not yours</td>\n",
       "      <td>In XXXX of 2019 I noticed a debt for {$620.00}...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CURO Intermediate Holdings</td>\n",
       "      <td>CO</td>\n",
       "      <td>806XX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Consent provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>3/12/2020</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3564184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2/6/2020</td>\n",
       "      <td>Vehicle loan or lease</td>\n",
       "      <td>Loan</td>\n",
       "      <td>Getting a loan or lease</td>\n",
       "      <td>Credit denial</td>\n",
       "      <td>As stated from Capital One, XXXX XX/XX/XXXX an...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CAPITAL ONE FINANCIAL CORPORATION</td>\n",
       "      <td>OH</td>\n",
       "      <td>430XX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Consent provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>2/6/2020</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3521949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3/6/2020</td>\n",
       "      <td>Checking or savings account</td>\n",
       "      <td>Savings account</td>\n",
       "      <td>Managing an account</td>\n",
       "      <td>Banking errors</td>\n",
       "      <td>Please see CFPB case XXXX. \\n\\nCapital One, in...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CAPITAL ONE FINANCIAL CORPORATION</td>\n",
       "      <td>CA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Consent provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>3/6/2020</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3556237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2/14/2020</td>\n",
       "      <td>Debt collection</td>\n",
       "      <td>Medical debt</td>\n",
       "      <td>Attempts to collect debt not owed</td>\n",
       "      <td>Debt is not yours</td>\n",
       "      <td>This debt was incurred due to medical malpract...</td>\n",
       "      <td>Company believes it acted appropriately as aut...</td>\n",
       "      <td>Merchants and Professional Bureau, Inc.</td>\n",
       "      <td>OH</td>\n",
       "      <td>432XX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Consent provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>2/14/2020</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3531704</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Date received                      Product       Sub-product  \\\n",
       "0      4/3/2020        Vehicle loan or lease              Loan   \n",
       "1     3/12/2020              Debt collection  Payday loan debt   \n",
       "2      2/6/2020        Vehicle loan or lease              Loan   \n",
       "3      3/6/2020  Checking or savings account   Savings account   \n",
       "4     2/14/2020              Debt collection      Medical debt   \n",
       "\n",
       "                               Issue          Sub-issue  \\\n",
       "0            Getting a loan or lease    Fraudulent loan   \n",
       "1  Attempts to collect debt not owed  Debt is not yours   \n",
       "2            Getting a loan or lease      Credit denial   \n",
       "3                Managing an account     Banking errors   \n",
       "4  Attempts to collect debt not owed  Debt is not yours   \n",
       "\n",
       "                        Consumer complaint narrative  \\\n",
       "0  This auto loan was opened on XX/XX/2020 in XXX...   \n",
       "1  In XXXX of 2019 I noticed a debt for {$620.00}...   \n",
       "2  As stated from Capital One, XXXX XX/XX/XXXX an...   \n",
       "3  Please see CFPB case XXXX. \\n\\nCapital One, in...   \n",
       "4  This debt was incurred due to medical malpract...   \n",
       "\n",
       "                             Company public response  \\\n",
       "0  Company has responded to the consumer and the ...   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4  Company believes it acted appropriately as aut...   \n",
       "\n",
       "                                   Company State ZIP code Tags  \\\n",
       "0             TRUIST FINANCIAL CORPORATION    PA      NaN  NaN   \n",
       "1               CURO Intermediate Holdings    CO    806XX  NaN   \n",
       "2        CAPITAL ONE FINANCIAL CORPORATION    OH    430XX  NaN   \n",
       "3        CAPITAL ONE FINANCIAL CORPORATION    CA      NaN  NaN   \n",
       "4  Merchants and Professional Bureau, Inc.    OH    432XX  NaN   \n",
       "\n",
       "  Consumer consent provided? Submitted via Date sent to company  \\\n",
       "0           Consent provided           Web             4/3/2020   \n",
       "1           Consent provided           Web            3/12/2020   \n",
       "2           Consent provided           Web             2/6/2020   \n",
       "3           Consent provided           Web             3/6/2020   \n",
       "4           Consent provided           Web            2/14/2020   \n",
       "\n",
       "  Company response to consumer Timely response?  Consumer disputed?  \\\n",
       "0      Closed with explanation              Yes                 NaN   \n",
       "1      Closed with explanation              Yes                 NaN   \n",
       "2      Closed with explanation              Yes                 NaN   \n",
       "3      Closed with explanation              Yes                 NaN   \n",
       "4      Closed with explanation              Yes                 NaN   \n",
       "\n",
       "   Complaint ID  \n",
       "0       3591341  \n",
       "1       3564184  \n",
       "2       3521949  \n",
       "3       3556237  \n",
       "4       3531704  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Take a loot at the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "excited-executive",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Value must be a nonnegative integer or None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## To be abe to read whole narrative\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_option\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdisplay.max_colwidth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\raj\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_config\\config.py:272\u001b[0m, in \u001b[0;36mCallableDynamicDoc.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m--> 272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__func__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\raj\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_config\\config.py:171\u001b[0m, in \u001b[0;36m_set_option\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    169\u001b[0m o \u001b[38;5;241m=\u001b[39m _get_registered_option(key)\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m o \u001b[38;5;129;01mand\u001b[39;00m o\u001b[38;5;241m.\u001b[39mvalidator:\n\u001b[1;32m--> 171\u001b[0m     \u001b[43mo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# walk the nested dict\u001b[39;00m\n\u001b[0;32m    174\u001b[0m root, k_root \u001b[38;5;241m=\u001b[39m _get_root(key)\n",
      "File \u001b[1;32mc:\\Users\\raj\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_config\\config.py:919\u001b[0m, in \u001b[0;36mis_nonnegative_int\u001b[1;34m(value)\u001b[0m\n\u001b[0;32m    916\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    918\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValue must be a nonnegative integer or None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 919\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: Value must be a nonnegative integer or None"
     ]
    }
   ],
   "source": [
    "## To be abe to read whole narrative\n",
    "# pd.set_option(\"display.max_colwidth\", -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outstanding-stream",
   "metadata": {},
   "source": [
    "- there are a lot of attributes which we are not interested in for the task in hand. We'll be working on Customer complaint narrative and try to perform topic modeling on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ethical-armstrong",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Useful columns\n",
    "df = df[['Product', 'Consumer complaint narrative']]\n",
    "df.columns = ['Product', 'Narrative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "rational-burner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vehicle loan or lease</td>\n",
       "      <td>This auto loan was opened on XX/XX/2020 in XXX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Debt collection</td>\n",
       "      <td>In XXXX of 2019 I noticed a debt for {$620.00}...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vehicle loan or lease</td>\n",
       "      <td>As stated from Capital One, XXXX XX/XX/XXXX an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Checking or savings account</td>\n",
       "      <td>Please see CFPB case XXXX. \\n\\nCapital One, in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Debt collection</td>\n",
       "      <td>This debt was incurred due to medical malpract...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Product  \\\n",
       "0        Vehicle loan or lease   \n",
       "1              Debt collection   \n",
       "2        Vehicle loan or lease   \n",
       "3  Checking or savings account   \n",
       "4              Debt collection   \n",
       "\n",
       "                                           Narrative  \n",
       "0  This auto loan was opened on XX/XX/2020 in XXX...  \n",
       "1  In XXXX of 2019 I noticed a debt for {$620.00}...  \n",
       "2  As stated from Capital One, XXXX XX/XX/XXXX an...  \n",
       "3  Please see CFPB case XXXX. \\n\\nCapital One, in...  \n",
       "4  This debt was incurred due to medical malpract...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "following-climate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57453, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "broad-lafayette",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Product\n",
       "Debt collection                21772\n",
       "Credit card or prepaid card    13193\n",
       "Mortgage                        9799\n",
       "Checking or savings account     7003\n",
       "Student loan                    2950\n",
       "Vehicle loan or lease           2736\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Product'].value_counts()\n",
    "## Class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matched-average",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "global-assurance",
   "metadata": {},
   "outputs": [],
   "source": [
    "## performes tokenization.\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "center-recipient",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Custom Tokenization:\n",
    "## Those card numbers with Xx in them are of no use. also, I am not interested in amount and other numerical features. \n",
    "def tokenize(sentance):\n",
    "    tokens = word_tokenize(sentance)\n",
    "    tokens = [word for word in tokens if ( word.isalpha() and len(word)>3 and len(word.strip('Xx/'))>2) ]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "straight-birmingham",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Used for splitting our data into train and test set.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = df[['Narrative']], df[['Product']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bored-daily",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Resolving class Imbalance\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "strategy = {\n",
    "    \"Debt collection\"              :  21772,\n",
    "    \"Credit card or prepaid card\"  :  13193,\n",
    "    \"Mortgage\"                     :  11000 ,\n",
    "    \"Checking or savings account\"  :  9000 ,\n",
    "    \"Student loan\"                 :  7000 ,\n",
    "    \"Vehicle loan or lease\"        :  7000 }\n",
    "\n",
    "ros = RandomOverSampler(sampling_strategy = strategy, random_state=42)\n",
    "X_train, y_train = ros.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "compatible-mileage",
   "metadata": {},
   "outputs": [],
   "source": [
    "## we need to represent words as numerical values/ vectors.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ancient-regard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mInit signature:\u001b[0m\n",
      "\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'content'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdecode_error\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'strict'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mstrip_accents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mlowercase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mpreprocessor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mtokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0manalyzer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'word'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mtoken_pattern\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'(?u)\\\\b\\\\w\\\\w+\\\\b'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mngram_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmax_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmin_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mvocabulary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;32mclass\u001b[0m \u001b[1;34m'numpy.float64'\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'l2'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0muse_idf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0msmooth_idf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0msublinear_tf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m     \n",
      "Convert a collection of raw documents to a matrix of TF-IDF features.\n",
      "\n",
      "Equivalent to :class:`CountVectorizer` followed by\n",
      ":class:`TfidfTransformer`.\n",
      "\n",
      "For an example of usage, see\n",
      ":ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`.\n",
      "\n",
      "For an efficiency comparision of the different feature extractors, see\n",
      ":ref:`sphx_glr_auto_examples_text_plot_hashing_vs_dict_vectorizer.py`.\n",
      "\n",
      "Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "input : {'filename', 'file', 'content'}, default='content'\n",
      "    - If `'filename'`, the sequence passed as an argument to fit is\n",
      "      expected to be a list of filenames that need reading to fetch\n",
      "      the raw content to analyze.\n",
      "\n",
      "    - If `'file'`, the sequence items must have a 'read' method (file-like\n",
      "      object) that is called to fetch the bytes in memory.\n",
      "\n",
      "    - If `'content'`, the input is expected to be a sequence of items that\n",
      "      can be of type string or byte.\n",
      "\n",
      "encoding : str, default='utf-8'\n",
      "    If bytes or files are given to analyze, this encoding is used to\n",
      "    decode.\n",
      "\n",
      "decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n",
      "    Instruction on what to do if a byte sequence is given to analyze that\n",
      "    contains characters not of the given `encoding`. By default, it is\n",
      "    'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
      "    values are 'ignore' and 'replace'.\n",
      "\n",
      "strip_accents : {'ascii', 'unicode'} or callable, default=None\n",
      "    Remove accents and perform other character normalization\n",
      "    during the preprocessing step.\n",
      "    'ascii' is a fast method that only works on characters that have\n",
      "    a direct ASCII mapping.\n",
      "    'unicode' is a slightly slower method that works on any characters.\n",
      "    None (default) means no character normalization is performed.\n",
      "\n",
      "    Both 'ascii' and 'unicode' use NFKD normalization from\n",
      "    :func:`unicodedata.normalize`.\n",
      "\n",
      "lowercase : bool, default=True\n",
      "    Convert all characters to lowercase before tokenizing.\n",
      "\n",
      "preprocessor : callable, default=None\n",
      "    Override the preprocessing (string transformation) stage while\n",
      "    preserving the tokenizing and n-grams generation steps.\n",
      "    Only applies if ``analyzer`` is not callable.\n",
      "\n",
      "tokenizer : callable, default=None\n",
      "    Override the string tokenization step while preserving the\n",
      "    preprocessing and n-grams generation steps.\n",
      "    Only applies if ``analyzer == 'word'``.\n",
      "\n",
      "analyzer : {'word', 'char', 'char_wb'} or callable, default='word'\n",
      "    Whether the feature should be made of word or character n-grams.\n",
      "    Option 'char_wb' creates character n-grams only from text inside\n",
      "    word boundaries; n-grams at the edges of words are padded with space.\n",
      "\n",
      "    If a callable is passed it is used to extract the sequence of features\n",
      "    out of the raw, unprocessed input.\n",
      "\n",
      "    .. versionchanged:: 0.21\n",
      "        Since v0.21, if ``input`` is ``'filename'`` or ``'file'``, the data\n",
      "        is first read from the file and then passed to the given callable\n",
      "        analyzer.\n",
      "\n",
      "stop_words : {'english'}, list, default=None\n",
      "    If a string, it is passed to _check_stop_list and the appropriate stop\n",
      "    list is returned. 'english' is currently the only supported string\n",
      "    value.\n",
      "    There are several known issues with 'english' and you should\n",
      "    consider an alternative (see :ref:`stop_words`).\n",
      "\n",
      "    If a list, that list is assumed to contain stop words, all of which\n",
      "    will be removed from the resulting tokens.\n",
      "    Only applies if ``analyzer == 'word'``.\n",
      "\n",
      "    If None, no stop words will be used. In this case, setting `max_df`\n",
      "    to a higher value, such as in the range (0.7, 1.0), can automatically detect\n",
      "    and filter stop words based on intra corpus document frequency of terms.\n",
      "\n",
      "token_pattern : str, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"\n",
      "    Regular expression denoting what constitutes a \"token\", only used\n",
      "    if ``analyzer == 'word'``. The default regexp selects tokens of 2\n",
      "    or more alphanumeric characters (punctuation is completely ignored\n",
      "    and always treated as a token separator).\n",
      "\n",
      "    If there is a capturing group in token_pattern then the\n",
      "    captured group content, not the entire match, becomes the token.\n",
      "    At most one capturing group is permitted.\n",
      "\n",
      "ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
      "    The lower and upper boundary of the range of n-values for different\n",
      "    n-grams to be extracted. All values of n such that min_n <= n <= max_n\n",
      "    will be used. For example an ``ngram_range`` of ``(1, 1)`` means only\n",
      "    unigrams, ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` means\n",
      "    only bigrams.\n",
      "    Only applies if ``analyzer`` is not callable.\n",
      "\n",
      "max_df : float or int, default=1.0\n",
      "    When building the vocabulary ignore terms that have a document\n",
      "    frequency strictly higher than the given threshold (corpus-specific\n",
      "    stop words).\n",
      "    If float in range [0.0, 1.0], the parameter represents a proportion of\n",
      "    documents, integer absolute counts.\n",
      "    This parameter is ignored if vocabulary is not None.\n",
      "\n",
      "min_df : float or int, default=1\n",
      "    When building the vocabulary ignore terms that have a document\n",
      "    frequency strictly lower than the given threshold. This value is also\n",
      "    called cut-off in the literature.\n",
      "    If float in range of [0.0, 1.0], the parameter represents a proportion\n",
      "    of documents, integer absolute counts.\n",
      "    This parameter is ignored if vocabulary is not None.\n",
      "\n",
      "max_features : int, default=None\n",
      "    If not None, build a vocabulary that only consider the top\n",
      "    `max_features` ordered by term frequency across the corpus.\n",
      "    Otherwise, all features are used.\n",
      "\n",
      "    This parameter is ignored if vocabulary is not None.\n",
      "\n",
      "vocabulary : Mapping or iterable, default=None\n",
      "    Either a Mapping (e.g., a dict) where keys are terms and values are\n",
      "    indices in the feature matrix, or an iterable over terms. If not\n",
      "    given, a vocabulary is determined from the input documents.\n",
      "\n",
      "binary : bool, default=False\n",
      "    If True, all non-zero term counts are set to 1. This does not mean\n",
      "    outputs will have only 0/1 values, only that the tf term in tf-idf\n",
      "    is binary. (Set `binary` to True, `use_idf` to False and\n",
      "    `norm` to None to get 0/1 outputs).\n",
      "\n",
      "dtype : dtype, default=float64\n",
      "    Type of the matrix returned by fit_transform() or transform().\n",
      "\n",
      "norm : {'l1', 'l2'} or None, default='l2'\n",
      "    Each output row will have unit norm, either:\n",
      "\n",
      "    - 'l2': Sum of squares of vector elements is 1. The cosine\n",
      "      similarity between two vectors is their dot product when l2 norm has\n",
      "      been applied.\n",
      "    - 'l1': Sum of absolute values of vector elements is 1.\n",
      "      See :func:`~sklearn.preprocessing.normalize`.\n",
      "    - None: No normalization.\n",
      "\n",
      "use_idf : bool, default=True\n",
      "    Enable inverse-document-frequency reweighting. If False, idf(t) = 1.\n",
      "\n",
      "smooth_idf : bool, default=True\n",
      "    Smooth idf weights by adding one to document frequencies, as if an\n",
      "    extra document was seen containing every term in the collection\n",
      "    exactly once. Prevents zero divisions.\n",
      "\n",
      "sublinear_tf : bool, default=False\n",
      "    Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).\n",
      "\n",
      "Attributes\n",
      "----------\n",
      "vocabulary_ : dict\n",
      "    A mapping of terms to feature indices.\n",
      "\n",
      "fixed_vocabulary_ : bool\n",
      "    True if a fixed vocabulary of term to indices mapping\n",
      "    is provided by the user.\n",
      "\n",
      "idf_ : array of shape (n_features,)\n",
      "    The inverse document frequency (IDF) vector; only defined\n",
      "    if ``use_idf`` is True.\n",
      "\n",
      "stop_words_ : set\n",
      "    Terms that were ignored because they either:\n",
      "\n",
      "      - occurred in too many documents (`max_df`)\n",
      "      - occurred in too few documents (`min_df`)\n",
      "      - were cut off by feature selection (`max_features`).\n",
      "\n",
      "    This is only available if no vocabulary was given.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "CountVectorizer : Transforms text into a sparse matrix of n-gram counts.\n",
      "\n",
      "TfidfTransformer : Performs the TF-IDF transformation from a provided\n",
      "    matrix of counts.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "The ``stop_words_`` attribute can get large and increase the model size\n",
      "when pickling. This attribute is provided only for introspection and can\n",
      "be safely removed using delattr or set to None before pickling.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> from sklearn.feature_extraction.text import TfidfVectorizer\n",
      ">>> corpus = [\n",
      "...     'This is the first document.',\n",
      "...     'This document is the second document.',\n",
      "...     'And this is the third one.',\n",
      "...     'Is this the first document?',\n",
      "... ]\n",
      ">>> vectorizer = TfidfVectorizer()\n",
      ">>> X = vectorizer.fit_transform(corpus)\n",
      ">>> vectorizer.get_feature_names_out()\n",
      "array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
      "       'this'], ...)\n",
      ">>> print(X.shape)\n",
      "(4, 9)\n",
      "\u001b[1;31mFile:\u001b[0m           c:\\users\\raj\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\n",
      "\u001b[1;31mType:\u001b[0m           type\n",
      "\u001b[1;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "## Very helpful indeed. Different libraries may have differet implementation of the same functionality.\n",
    "## It is useful to take a look at the parameters of the function\n",
    "TfidfVectorizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "peaceful-applicant",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\raj\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer = tokenize, stop_words = 'english', \n",
    "                            max_df = 0.75, min_df = 50, max_features = 10000)\n",
    "vectors = vectorizer.fit_transform(X_train['Narrative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dominican-spine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68965, 4943)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cardiac-cathedral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aadvantage', 'abandoned', 'abide', ..., 'youre', 'zero', 'zone'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Feature names mapped to the array index.\n",
    "## 1st feature in vectors is the word at 1 index of the list.\n",
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifth-benjamin",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eastern-forth",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "occupational-classic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mInit signature:\u001b[0m\n",
      "\u001b[0mLatentDirichletAllocation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdoc_topic_prior\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mtopic_word_prior\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mlearning_method\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'batch'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mlearning_decay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mlearning_offset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mevaluate_every\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mtotal_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000000.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mperp_tol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmean_change_tol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmax_doc_update_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m     \n",
      "Latent Dirichlet Allocation with online variational Bayes algorithm.\n",
      "\n",
      "The implementation is based on [1]_ and [2]_.\n",
      "\n",
      ".. versionadded:: 0.17\n",
      "\n",
      "Read more in the :ref:`User Guide <LatentDirichletAllocation>`.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "n_components : int, default=10\n",
      "    Number of topics.\n",
      "\n",
      "    .. versionchanged:: 0.19\n",
      "        ``n_topics`` was renamed to ``n_components``\n",
      "\n",
      "doc_topic_prior : float, default=None\n",
      "    Prior of document topic distribution `theta`. If the value is None,\n",
      "    defaults to `1 / n_components`.\n",
      "    In [1]_, this is called `alpha`.\n",
      "\n",
      "topic_word_prior : float, default=None\n",
      "    Prior of topic word distribution `beta`. If the value is None, defaults\n",
      "    to `1 / n_components`.\n",
      "    In [1]_, this is called `eta`.\n",
      "\n",
      "learning_method : {'batch', 'online'}, default='batch'\n",
      "    Method used to update `_component`. Only used in :meth:`fit` method.\n",
      "    In general, if the data size is large, the online update will be much\n",
      "    faster than the batch update.\n",
      "\n",
      "    Valid options::\n",
      "\n",
      "        'batch': Batch variational Bayes method. Use all training data in\n",
      "            each EM update.\n",
      "            Old `components_` will be overwritten in each iteration.\n",
      "        'online': Online variational Bayes method. In each EM update, use\n",
      "            mini-batch of training data to update the ``components_``\n",
      "            variable incrementally. The learning rate is controlled by the\n",
      "            ``learning_decay`` and the ``learning_offset`` parameters.\n",
      "\n",
      "    .. versionchanged:: 0.20\n",
      "        The default learning method is now ``\"batch\"``.\n",
      "\n",
      "learning_decay : float, default=0.7\n",
      "    It is a parameter that control learning rate in the online learning\n",
      "    method. The value should be set between (0.5, 1.0] to guarantee\n",
      "    asymptotic convergence. When the value is 0.0 and batch_size is\n",
      "    ``n_samples``, the update method is same as batch learning. In the\n",
      "    literature, this is called kappa.\n",
      "\n",
      "learning_offset : float, default=10.0\n",
      "    A (positive) parameter that downweights early iterations in online\n",
      "    learning.  It should be greater than 1.0. In the literature, this is\n",
      "    called tau_0.\n",
      "\n",
      "max_iter : int, default=10\n",
      "    The maximum number of passes over the training data (aka epochs).\n",
      "    It only impacts the behavior in the :meth:`fit` method, and not the\n",
      "    :meth:`partial_fit` method.\n",
      "\n",
      "batch_size : int, default=128\n",
      "    Number of documents to use in each EM iteration. Only used in online\n",
      "    learning.\n",
      "\n",
      "evaluate_every : int, default=-1\n",
      "    How often to evaluate perplexity. Only used in `fit` method.\n",
      "    set it to 0 or negative number to not evaluate perplexity in\n",
      "    training at all. Evaluating perplexity can help you check convergence\n",
      "    in training process, but it will also increase total training time.\n",
      "    Evaluating perplexity in every iteration might increase training time\n",
      "    up to two-fold.\n",
      "\n",
      "total_samples : int, default=1e6\n",
      "    Total number of documents. Only used in the :meth:`partial_fit` method.\n",
      "\n",
      "perp_tol : float, default=1e-1\n",
      "    Perplexity tolerance in batch learning. Only used when\n",
      "    ``evaluate_every`` is greater than 0.\n",
      "\n",
      "mean_change_tol : float, default=1e-3\n",
      "    Stopping tolerance for updating document topic distribution in E-step.\n",
      "\n",
      "max_doc_update_iter : int, default=100\n",
      "    Max number of iterations for updating document topic distribution in\n",
      "    the E-step.\n",
      "\n",
      "n_jobs : int, default=None\n",
      "    The number of jobs to use in the E-step.\n",
      "    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "    for more details.\n",
      "\n",
      "verbose : int, default=0\n",
      "    Verbosity level.\n",
      "\n",
      "random_state : int, RandomState instance or None, default=None\n",
      "    Pass an int for reproducible results across multiple function calls.\n",
      "    See :term:`Glossary <random_state>`.\n",
      "\n",
      "Attributes\n",
      "----------\n",
      "components_ : ndarray of shape (n_components, n_features)\n",
      "    Variational parameters for topic word distribution. Since the complete\n",
      "    conditional for topic word distribution is a Dirichlet,\n",
      "    ``components_[i, j]`` can be viewed as pseudocount that represents the\n",
      "    number of times word `j` was assigned to topic `i`.\n",
      "    It can also be viewed as distribution over the words for each topic\n",
      "    after normalization:\n",
      "    ``model.components_ / model.components_.sum(axis=1)[:, np.newaxis]``.\n",
      "\n",
      "exp_dirichlet_component_ : ndarray of shape (n_components, n_features)\n",
      "    Exponential value of expectation of log topic word distribution.\n",
      "    In the literature, this is `exp(E[log(beta)])`.\n",
      "\n",
      "n_batch_iter_ : int\n",
      "    Number of iterations of the EM step.\n",
      "\n",
      "n_features_in_ : int\n",
      "    Number of features seen during :term:`fit`.\n",
      "\n",
      "    .. versionadded:: 0.24\n",
      "\n",
      "feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "    Names of features seen during :term:`fit`. Defined only when `X`\n",
      "    has feature names that are all strings.\n",
      "\n",
      "    .. versionadded:: 1.0\n",
      "\n",
      "n_iter_ : int\n",
      "    Number of passes over the dataset.\n",
      "\n",
      "bound_ : float\n",
      "    Final perplexity score on training set.\n",
      "\n",
      "doc_topic_prior_ : float\n",
      "    Prior of document topic distribution `theta`. If the value is None,\n",
      "    it is `1 / n_components`.\n",
      "\n",
      "random_state_ : RandomState instance\n",
      "    RandomState instance that is generated either from a seed, the random\n",
      "    number generator or by `np.random`.\n",
      "\n",
      "topic_word_prior_ : float\n",
      "    Prior of topic word distribution `beta`. If the value is None, it is\n",
      "    `1 / n_components`.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "sklearn.discriminant_analysis.LinearDiscriminantAnalysis:\n",
      "    A classifier with a linear decision boundary, generated by fitting\n",
      "    class conditional densities to the data and using Bayes' rule.\n",
      "\n",
      "References\n",
      "----------\n",
      ".. [1] \"Online Learning for Latent Dirichlet Allocation\", Matthew D.\n",
      "       Hoffman, David M. Blei, Francis Bach, 2010\n",
      "       https://github.com/blei-lab/onlineldavb\n",
      "\n",
      ".. [2] \"Stochastic Variational Inference\", Matthew D. Hoffman,\n",
      "       David M. Blei, Chong Wang, John Paisley, 2013\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> from sklearn.decomposition import LatentDirichletAllocation\n",
      ">>> from sklearn.datasets import make_multilabel_classification\n",
      ">>> # This produces a feature matrix of token counts, similar to what\n",
      ">>> # CountVectorizer would produce on text.\n",
      ">>> X, _ = make_multilabel_classification(random_state=0)\n",
      ">>> lda = LatentDirichletAllocation(n_components=5,\n",
      "...     random_state=0)\n",
      ">>> lda.fit(X)\n",
      "LatentDirichletAllocation(...)\n",
      ">>> # get topics for some given samples:\n",
      ">>> lda.transform(X[-2:])\n",
      "array([[0.00360392, 0.25499205, 0.0036211 , 0.64236448, 0.09541846],\n",
      "       [0.15297572, 0.00362644, 0.44412786, 0.39568399, 0.003586  ]])\n",
      "\u001b[1;31mFile:\u001b[0m           c:\\users\\raj\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\sklearn\\decomposition\\_lda.py\n",
      "\u001b[1;31mType:\u001b[0m           type\n",
      "\u001b[1;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "LatentDirichletAllocation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "structured-boston",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components = 6, learning_method = 'online', max_iter = 100,\n",
    "                         n_jobs = -1, random_state = 123)\n",
    "\n",
    "W1 = lda.fit_transform(vectors)\n",
    "\n",
    "H1 = lda.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "labeled-feeling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03826295, 0.01854838, 0.03993724, 0.09456617, 0.01835607,\n",
       "        0.79032918],\n",
       "       [0.03284839, 0.03272191, 0.03276249, 0.03349488, 0.0333319 ,\n",
       "        0.83484044],\n",
       "       [0.02750916, 0.0478809 , 0.0276351 , 0.02772895, 0.02712774,\n",
       "        0.84211816],\n",
       "       ...,\n",
       "       [0.03619558, 0.03749331, 0.03575796, 0.38915436, 0.03611963,\n",
       "        0.46527916],\n",
       "       [0.02123505, 0.02122891, 0.89386232, 0.02122752, 0.02122279,\n",
       "        0.02122342],\n",
       "       [0.25284468, 0.03635479, 0.03596977, 0.60240052, 0.0361438 ,\n",
       "        0.03628645]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## H1\n",
    "W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "surgical-trader",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68965, 6) (6, 4943)\n"
     ]
    }
   ],
   "source": [
    "## W1 -> Probability of document being of the topic(i). 6 topics->6 probabilities.\n",
    "## H1 -> Topic-> Words\n",
    "print(W1.shape, H1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "graduate-application",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "## Gives last (numwords-1) numbers in descending order. from n to 0.\n",
    "## So, for each topic, we get top 10 words that appear in that topic.\n",
    "def get_top_words(words):\n",
    "    res = []\n",
    "    for index in np.argsort(words)[:-10:-1]:\n",
    "        res.append(vocab[index])\n",
    "    return res\n",
    "## top_words = lambda words: [vocab[index] for index in np.argsort(words)[:-top:-1]]\n",
    "\n",
    "## Get topic words using H1.\n",
    "topic_words = (get_top_words(words) for words in H1)\n",
    "topics = [','.join(t) for t in topic_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "english-voltage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loan,loans,payments,student,navient,payment,forbearance,rate,income',\n",
       " 'mortgage,escrow,loan,modification,property,insurance,home,foreclosure,servicing',\n",
       " 'usaa,bonus,opened,promotion,checks,suntrust,union,credit,relation',\n",
       " 'debt,credit,collection,company,account,report,information,letter,reporting',\n",
       " 'identity,theft,victim,belong,report,affidavit,debt,santander,does',\n",
       " 'account,bank,card,payment,credit,told,called,money,said']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics\n",
    "## These are based on my intuition. Can be interpreted entirely differently as well.\n",
    "## 0 -> Student Loans\n",
    "## 1 -> Mortgage\n",
    "## 2 -> Credit card or prepaid card\n",
    "## 3 -> Debt Collection\n",
    "## 4 -> Vehicle loan or lease\n",
    "## 5 -> Checking or savings account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "alive-exploration",
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = [\"Topic_\" + str(i) for i in range(lda.n_components)]\n",
    "df1 = pd.DataFrame(np.round(W1, 2), columns = colnames, index = X_train.index)\n",
    "# significant_topic = np.argmax(df1.values, axis = 1)\n",
    "df1[\"Dominant Topic\"] = np.argmax(df1.values, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "absolute-uncle",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = X_train.join(df1)\n",
    "df1 = df1.join(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "executed-drawing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Narrative</th>\n",
       "      <th>Topic_0</th>\n",
       "      <th>Topic_1</th>\n",
       "      <th>Topic_2</th>\n",
       "      <th>Topic_3</th>\n",
       "      <th>Topic_4</th>\n",
       "      <th>Topic_5</th>\n",
       "      <th>Dominant Topic</th>\n",
       "      <th>Product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>On ( date ) I contacted Bank of America in reg...</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.79</td>\n",
       "      <td>5</td>\n",
       "      <td>Checking or savings account</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I had a issue with capital one buy power card ...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.83</td>\n",
       "      <td>5</td>\n",
       "      <td>Credit card or prepaid card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>On XX/XX/2019, 20 fraudulent charges were made...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.84</td>\n",
       "      <td>5</td>\n",
       "      <td>Credit card or prepaid card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Started a Credit Card through XXXX XXXX with X...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.82</td>\n",
       "      <td>5</td>\n",
       "      <td>Credit card or prepaid card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I went in to a Union Bank located in XXXX XXXX...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.79</td>\n",
       "      <td>5</td>\n",
       "      <td>Checking or savings account</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Narrative  Topic_0  Topic_1  \\\n",
       "0  On ( date ) I contacted Bank of America in reg...     0.04     0.02   \n",
       "1  I had a issue with capital one buy power card ...     0.03     0.03   \n",
       "2  On XX/XX/2019, 20 fraudulent charges were made...     0.03     0.05   \n",
       "3  Started a Credit Card through XXXX XXXX with X...     0.03     0.03   \n",
       "4  I went in to a Union Bank located in XXXX XXXX...     0.02     0.02   \n",
       "\n",
       "   Topic_2  Topic_3  Topic_4  Topic_5  Dominant Topic  \\\n",
       "0     0.04     0.09     0.02     0.79               5   \n",
       "1     0.03     0.03     0.03     0.83               5   \n",
       "2     0.03     0.03     0.03     0.84               5   \n",
       "3     0.03     0.06     0.03     0.82               5   \n",
       "4     0.07     0.08     0.02     0.79               5   \n",
       "\n",
       "                       Product  \n",
       "0  Checking or savings account  \n",
       "1  Credit card or prepaid card  \n",
       "2  Credit card or prepaid card  \n",
       "3  Credit card or prepaid card  \n",
       "4  Checking or savings account  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "least-proxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = lda.transform(vectorizer.transform(X_test['Narrative']))\n",
    "\n",
    "X_test[\"Dominant Topic\"] = np.argmax(W2, axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "abstract-affairs",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Narrative</th>\n",
       "      <th>Dominant Topic</th>\n",
       "      <th>Product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40496</th>\n",
       "      <td>Our mortgage loan is serviced by The Money Sou...</td>\n",
       "      <td>5</td>\n",
       "      <td>Mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17199</th>\n",
       "      <td>To Whom It May Concern : This letter is regard...</td>\n",
       "      <td>3</td>\n",
       "      <td>Debt collection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45428</th>\n",
       "      <td>I started receiving calls from Portfolio Recov...</td>\n",
       "      <td>3</td>\n",
       "      <td>Debt collection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56085</th>\n",
       "      <td>This company, Credence Resource Management is ...</td>\n",
       "      <td>3</td>\n",
       "      <td>Debt collection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41199</th>\n",
       "      <td>I shopped at Nordstrom rack on XX/XX/XXXX, and...</td>\n",
       "      <td>5</td>\n",
       "      <td>Credit card or prepaid card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35599</th>\n",
       "      <td>In XXXX I filed complaint number XXXX. I also ...</td>\n",
       "      <td>5</td>\n",
       "      <td>Mortgage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17220</th>\n",
       "      <td>I recently started to look into my credit back...</td>\n",
       "      <td>3</td>\n",
       "      <td>Debt collection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14443</th>\n",
       "      <td>This company ( National credit Adjusters ) whi...</td>\n",
       "      <td>3</td>\n",
       "      <td>Debt collection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27947</th>\n",
       "      <td>I present the complaint of the collection agen...</td>\n",
       "      <td>3</td>\n",
       "      <td>Debt collection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>I moved out of my old apartment complex before...</td>\n",
       "      <td>3</td>\n",
       "      <td>Debt collection</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Narrative  Dominant Topic  \\\n",
       "40496  Our mortgage loan is serviced by The Money Sou...               5   \n",
       "17199  To Whom It May Concern : This letter is regard...               3   \n",
       "45428  I started receiving calls from Portfolio Recov...               3   \n",
       "56085  This company, Credence Resource Management is ...               3   \n",
       "41199  I shopped at Nordstrom rack on XX/XX/XXXX, and...               5   \n",
       "35599  In XXXX I filed complaint number XXXX. I also ...               5   \n",
       "17220  I recently started to look into my credit back...               3   \n",
       "14443  This company ( National credit Adjusters ) whi...               3   \n",
       "27947  I present the complaint of the collection agen...               3   \n",
       "172    I moved out of my old apartment complex before...               3   \n",
       "\n",
       "                           Product  \n",
       "40496                     Mortgage  \n",
       "17199              Debt collection  \n",
       "45428              Debt collection  \n",
       "56085              Debt collection  \n",
       "41199  Credit card or prepaid card  \n",
       "35599                     Mortgage  \n",
       "17220              Debt collection  \n",
       "14443              Debt collection  \n",
       "27947              Debt collection  \n",
       "172                Debt collection  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.join(y_test).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-france",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
